```{r setup, include=FALSE}
options(width=100)
options(digits = 4)
set.seed(12345)
```

---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(ggplot2)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *

Exploring the dependant variable - Price  

```{r price-distribution}
plot_price <- ggplot(ames_train, aes(x=price)) +
  geom_histogram(bins = 30, fill = "green", colour = "black") +
  labs(title = "Price Distribution Histogram", x = "Price", y = "Count") +
  geom_vline(xintercept = median(ames_train$price), linetype="dashed", size=1.5) +
  geom_text(x=median(ames_train$price) + 150000, y=150, label=paste("Median: ", median(ames_train$price)))

plot_log_price <- ggplot(ames_train, aes(log(price))) +
  geom_histogram(bins = 30, fill = "green", colour = "black") +
  labs(title = "Log Price Distribution Histogram", x = "Log of Price", y = "Count") +
  geom_vline(xintercept = median(log(ames_train$price)), linetype="dashed", size=1.5) +
  geom_text(x=median(log(ames_train$price)) + 0.75, y=150, label=paste("Median: ", round(median(log(ames_train$price)), 2)))

gridExtra::grid.arrange(plot_price, plot_log_price, ncol = 2)
```

On the first graph:  

- The distribution is right-skewed and unimodal.  
- 50% of the houses were sold for \$159,467 or less. There are some properties that were sold for more than \$600,000.  
- With the skewness and some potential outliers, it may not be suitable for a regression model. To resolve the skewness, log of price will be used instead.  

On the second graph:  

- The skewness is gone. And the distribution is still unimodal.  
- There are still some outliers. Maybe some houses were sold under-priced.  

* * * 

In the dataset, there are some records having Sale.Condition as "Abnormal". Let's check if the records with Sale.Condition isn't Normal are the outliers in question, and if they really are under-priced.  
One way is check if the sold price is reasonable is to pair the price with a predictor variable. Area was chosen since it usually is a definite factor to pricing a house.  

```{r price_area}
m_price_area <- lm(log(price) ~ area, data = ames_train)

ggplot(ames_train, aes(x = area, y = log(price), shape = Sale.Condition, color = Sale.Condition)) +
  geom_point() +
  geom_abline(intercept = m_price_area$coefficients["(Intercept)"], slope = m_price_area$coefficients["area"], size = 1.5, linetype = "dashed") +
  labs(title = "Log of Price over Area - For each Sale Condition", x = "Area (Square Feet)", y = "Log of Price")
```

While most of "Normal" houses gather around the regression line, other Sale.Condition mostly lie under (indicating potential under-priced).  
To avoid feeding inaccurate data into the regression model, we will only consider "Normal" sales from now.

```{r log-price-distribution-normal}
ames_train_normal <- ames_train %>%
  mutate(Log.Price = log(price)) %>%
  filter(Sale.Condition == "Normal")
ggplot(ames_train_normal, aes(log(price))) +
  geom_histogram(bins = 30, fill = "green", colour = "black") +
  labs(title = "Log Price Distribution Histogram - Normal Sales", x = "Log of Price", y = "Count") +
  geom_vline(xintercept = median(ames_train_normal$Log.Price), linetype="dashed", size=1.5) +
  geom_text(x=median(ames_train_normal$Log.Price) + 0.34, y=90, label=paste("Median: ", round(median(ames_train_normal$Log.Price), 4)))
```

After removing the "Not-Normal" houses, the histogram became nicely centered. 

* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

Initial Model:  
Response Variable: Log.Price  
Predictors:  

  - area: the bigger the house the better   
  - Lot.Area: big lot means space for extension  
  - MS.SubClass: type property might effect the price  
  - Overall.Qual: well-built houses are usually sold for higher  
  - Overall.Cond: well-kept houses are usually sold for higher  
  - Age (Since Year.Remod.Add to Yr.Sold): the older a house the less people willing to buy  
  - Total.Bsmt.SF: more space is always welcome     
  - Bedroom.AbvGr: bedroom, the most important kind of room in a house  
  - Full.Baths (Bsmt.Full.Bath and Full.Bath): a well-designed house will have a reasonable retio of bedrooms and bathrooms    
  - Garage.Area: essential for those who has cars or again, more space is always welcome  

**Note on MS.SubClass**  

| SubClass | Description                                            |
| -------- | ------------------------------------------------------ |
|  20      |  1-STORY 1946 & NEWER ALL STYLES                       |
|  30      |  1-STORY 1945 & OLDER                                  |
|  40      |  1-STORY W/FINISHED ATTIC ALL AGES                     |
|  45      |  1-1/2 STORY - UNFINISHED ALL AGES                     |
|  50      |  1-1/2 STORY FINISHED ALL AGES                         |
|  60      |  2-STORY 1946 & NEWER                                  |
|  70      |  2-STORY 1945 & OLDER                                  | 
|  75      |  2-1/2 STORY ALL AGES                                  |
|  80      |  SPLIT OR MULTI-LEVEL                                  |
|  85      |  SPLIT FOYER                                           |
|  90      |  DUPLEX - ALL STYLES AND AGES                          |
| 120      |  1-STORY PUD (Planned Unit Development) - 1946 & NEWER |
| 150      |  1-1/2 STORY PUD - ALL AGES                            |
| 160      |  2-STORY PUD - 1946 & NEWER                            |
| 180      |  PUD - MULTILEVEL - INCL SPLIT LEV/FOYER               |
| 190      |  2 FAMILY CONVERSION - ALL STYLES AND AGES             |
 

```{r prepare-data}
prepare_data_func <- function(data) {
  data_changed <- data %>%
    filter(Sale.Condition == "Normal") %>%
    mutate(Log.Price = log(price)) %>%
    mutate(MS.SubClass = factor(MS.SubClass)) %>%
    mutate(Age.Sold = Yr.Sold - Year.Remod.Add) %>%
    mutate(Full.Baths = Bsmt.Full.Bath +  Full.Bath) %>%
    mutate(Bath.Bed.Ratio = ifelse(Full.Baths == 0 | Bedroom.AbvGr == 0, 0, Full.Baths / Bedroom.AbvGr)) %>%
    mutate(Porch.SF = Wood.Deck.SF + Open.Porch.SF + Enclosed.Porch, X3Ssn.Porch, Screen.Porch) %>%
    mutate(Neighborhood.Avg = ave(Log.Price, Neighborhood)) %>%
    mutate(MS.SubClass.Avg = ave(Log.Price, MS.SubClass))
    
    return(data_changed)
}

ames_train_edited <- prepare_data_func(ames_train)
```


```{r fit_model}
m_initial <- bas.lm(Log.Price ~ area + Lot.Area + MS.SubClass + Overall.Qual + Overall.Cond + Age.Sold + 
                      Total.Bsmt.SF + Bedroom.AbvGr + Full.Baths + Garage.Area, 
                    data = ames_train_edited,
                    prior = "AIC", 
                    modelprior = uniform(),
                    na.action = "na.omit")
```


```{r summary-of-init-model}
summary(m_initial)
confint(coefficients(m_initial))
```

All the variables are significant predictors.  
And the initial model is fairly good with the R2 score more than 0.9.  

* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

BAS provide 4 options to choose model:  

- HPM: the highest probability model
- MPM: the median probability model
- BMA: an average over all the models
- BPM: the single model with predictions closest to those obtained from BMA

Since our purpose is the best model for predicting house price, BMA and BPM will be considered.  

```{r model_select}
bma_model <- predict(m_initial, estimator="BMA")
variable.names(bma_model)

bpm_model <- predict(m_initial, estimator="BPM")
variable.names(bpm_model)
```

The two methods result in the same model.  
This means all of the chosen variables are significant in predicting the price.  

* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *

```{r model_resid}
ggplot(as.data.frame(cbind(id = ames_train_edited$PID, pred = bma_model$fit, resid = ames_train_edited$Log.Price - bma_model$fit)), aes(pred, resid)) +
  geom_point(colour = "blue", shape = 1) +
  labs(title = "BMA Model residual plot", x = "Predicted Log price", y = "Residuals") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_text(aes(label=ifelse(abs(resid)>0.5,as.character(id),'')),hjust=-0.1,vjust=0)
```

There is no pattern in the scatter plot, no fan shape found. Residuals lie randomly around the hline (y = 0).  
Most of the residuals is closed to 0. Except for some outliers (1 house (PID 911102170) with residual more than 0.75).  

* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *

To undo the Log transformation, use exp method to change the response variable back to the original units (dollars)

```{r model_rmse}
sqrt(mean((exp(bma_model$fit) - ames_train_edited$price)^2))
```

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *

Steps:  

- Prepare the test data similarly to what we did with the train data  
- Generate prediction for the test dataset, using the initial model  
- Calculate the RMSE score and compare with the score of bma_model  

```{r initmodel_test}
ames_test_edited <- prepare_data_func(ames_test)
bma_test <- predict(m_initial, newdata = ames_test_edited, estimator = "BMA")
sqrt(mean((exp(bma_test$fit) - ames_test_edited$price)^2))
```

The RMSE scores aren't differ much (20900) to the score on sample data (20249).  
Our model isn't overfitted.  

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

NOTE: Write your written response to section 3.1 here. Delete this note before you submit your work.

```{r data-final}
ames_train_edited <- prepare_data_func(ames_train)
ames_test_edited <- prepare_data_func(ames_test)
```

```{r model_playground}
m_initial <- bas.lm(Log.Price ~ area + Lot.Area + Total.Bsmt.SF + Garage.Area + 
                      Overall.Qual + Overall.Cond + Age.Sold +  
                      Neighborhood.Avg + MS.SubClass.Avg + 
                      Bath.Bed.Ratio + Bedroom.AbvGr + Kitchen.AbvGr + 
                      Fireplaces + Pool.Area + Porch.SF + Paved.Drive, 
                    data = ames_train_edited,
                    prior = "AIC", 
                    modelprior = uniform(),
                    na.action = "na.omit")
```

```{r summary-table}
summary(m_initial)
confint(coef(m_initial))
```

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *

One variable was transformed: Price, using Log.  
Also, "Not-Normal" records was removed.  
All these actions was to archive normality in response variable.  

* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

Yes. Interation variables: Bath.Bed.Ratio and Age.Sold.  
Bath.Bed.Ratio:  
Calculated by dividing Number of Bathrooms with Number of Bedrooms.  
The purpose was to explore the ratio of Bathrooms and Bedrooms.  
For example: a house with bathrooms equal bedrooms will be more valuable than a house, say, with only one bathroom and 5 bedrooms.  

Age.Sold:  
Calculated by substracting Yr.Sold by Year.Remod.Add.  
The purpose was to generalize the Age of houses. Since the price is when the house was sold.  

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

Since I don't have any experience in Real Estate business, base knowledge will be from experts from the field (real estate websites).  
A house's value is often based on the following attributes:  

- Location  
- Size  
- Interior (Bedroom, Bathroom, etc.)  
- Condition  
- Other amenities (Pool, Fireplace, etc.)

All variables that have connection to those attributes will be put into the model.  
After building a model with those variables and observing the result, it's optional to add, remove or transform some variables. Repeat this step until satisfy.  

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

The purpose of testing the model with out-of-sample data was to avoid overfitting.  
RMSE (root mean square error) was used to evaluate the model.  
if RMSE on out-of-sample data was significantly larger than sample data, the model is overfitted.  
The list of predictors will have to change for better generalize the model.  

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

```{r model_resid_plot}
bma_model <- predict(m_initial, estimator = "BMA")

ggplot(as.data.frame(cbind(id = ames_train_edited$PID, pred = bma_model$fit, resid = ames_train_edited$Log.Price - bma_model$fit)), aes(pred, resid)) +
  geom_point(colour = "blue", shape = 1) +
  labs(title = "BMA Model residual plot", x = "Predicted Log price", y = "Residuals") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_text(aes(label=ifelse(abs(resid)>0.5,as.character(id),'')),hjust=-0.1,vjust=0)
```

There is no pattern in the scatter plot, no fan shape found. Residuals lie randomly around the hline (y = 0).  
Most of the residuals is closed to 0. Except for some outliers (1 house (PID 911102170) with residual near 0.8).  

* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *

```{r model-RMSE}
sqrt(mean((exp(bma_model$fit) - ames_train_edited$price)^2))

bma_test <- predict(m_initial, newdata = ames_test_edited, estimator = "BMA")
sqrt(mean((exp(bma_test$fit) - ames_test_edited$price)^2))
```

The RMSE scores are closed for sample data (19521) and out-of-sample data (19540).  
The model is generalized well. No overfitting problem found.  

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

**Strength:**  

- High R2 score. The model works well with current data set.  
- Highly generalized. The model isn't overfitted and can performs well on out-of-sample data.  

**Weakness:**  

- Lack of insider knowledge. The model still can't solve the outlier.  

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

* * *

```{r prepare-validation-data}
ames_validation <- ames_validation %>%
  mutate(Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath), 0, Bsmt.Full.Bath))

ames_validation_edited <- prepare_data_func(ames_validation)
```

```{r model_validate}
bma_validation <- predict(m_initial, newdata = ames_validation_edited, estimator = "BMA", se.fit = TRUE)
sqrt(mean((exp(bma_validation$fit) - ames_validation_edited$price)^2))
```

RMSE score for validation data (20299) is slightly higher than sample (19521) and out-of-sample data (19540).

```{r validation-intervals}
pred_ci <- exp(confint(bma_validation))
mean(ames_validation$price > pred_ci[,1] &
       ames_validation$price < pred_ci[,2])
```
The model's coverage probability is 93.97\%, meaning that 95\% prediction interval for `price` include the true value of `price` roughly 93.97\% of the time.  
Although, ideally, the coverage probability should be greater than 95\%. The model result doesn't fall far off.  
We can say that our model reflect uncertainty pretty well.  

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

The model performs fairly well on ames data set. With R2 score of more than 0.92.  
But there is still rooms for improvements:  

- The RMSE is still high.  
- There are potential outliers that the model can't resolve.  
- The coverage probability is still below 95%.  

The shortcomings are from lacking of domain knowledge. If SMEs are available, they can:  

- Evaluate questionable records and provide valuable insights. From there, those records can be edited or removed accordingly, or some definite factors can be discovered.  
- Evaluate the model for adding or removing predictors.  

* * *
